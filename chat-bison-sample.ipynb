{"cells":[{"cell_type":"markdown","metadata":{"id":"N5afkyDMSBW5"},"source":["### Install Vertex AI SDK"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":794},"executionInfo":{"elapsed":6113,"status":"ok","timestamp":1694419228627,"user":{"displayName":"정명훈 Jerry","userId":"05756609176337112861"},"user_tz":-540},"id":"kc4WxYmLSBW5","outputId":"53057f01-50e8-46cc-822c-050f1cd6a64e"},"outputs":[],"source":["# Remove remark to install packages\n","# !pip install -U google-cloud-aiplatform --user"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1694419280530,"user":{"displayName":"정명훈 Jerry","userId":"05756609176337112861"},"user_tz":-540},"id":"LCaCx6PLSBW6"},"outputs":[],"source":["# Remove remark to authenticate\n","# from google.colab import auth\n","# auth.authenticate_user()"]},{"cell_type":"markdown","metadata":{"id":"BuQwwRiniVFG"},"source":["### Import libraries"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":10226,"status":"ok","timestamp":1694419293054,"user":{"displayName":"정명훈 Jerry","userId":"05756609176337112861"},"user_tz":-540},"id":"4zjV4alsiVql"},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-09-11 18:48:32.759230: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"]}],"source":["from vertexai.preview.language_models import TextGenerationModel,\\\n","                                            ChatModel,\\\n","                                            InputOutputTextPair,\\\n","                                            TextEmbeddingModel\n","\n","from IPython.display import display, Markdown\n","import pandas as pd\n","from sklearn.metrics.pairwise import cosine_similarity\n","import seaborn as sns\n","\n","PROJECT_ID = \"jerry-argolis\"\n","LOCATION = \"us-central1\"\n","\n","# Initialize Vertex AI SDK\n","import vertexai\n","vertexai.init(project=PROJECT_ID, location=LOCATION)"]},{"cell_type":"markdown","metadata":{"id":"5sOdog1Poigr"},"source":["## Chat model with `chat-bison@001`"]},{"cell_type":"markdown","metadata":{"id":"Pb7hAFfFoigr"},"source":["The `chat-bison@001` model lets you have a freeform conversation across multiple turns. The application tracks what was previously said in the conversation. As such, if you expect to use conversations in your application, use the `chat-bison@001` model because it has been fine-tuned for multi-turn conversation use cases."]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1354,"status":"ok","timestamp":1694419811369,"user":{"displayName":"정명훈 Jerry","userId":"05756609176337112861"},"user_tz":-540},"id":"pMnPttMioigs","outputId":"19f0ff24-8c3f-499c-c5e7-5690c51221c9"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'vertexai.preview.language_models._PreviewChatModel'>\n","<class 'vertexai.language_models.ChatSession'>\n","Hi Larry, how can I help you today?\n"]}],"source":["chat_model = ChatModel.from_pretrained(\"chat-bison@001\")\n","print(type(chat_model))\n","\n","chat = chat_model.start_chat()\n","print(type(chat))\n","\n","print(chat.send_message(\"\"\"\n","My name is Larry.\n","\"\"\"))"]},{"cell_type":"markdown","metadata":{"id":"3O4dKx12oigs"},"source":["As shown below, the model should respond based on what was previously said in the conversation:"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1136,"status":"ok","timestamp":1694419821084,"user":{"displayName":"정명훈 Jerry","userId":"05756609176337112861"},"user_tz":-540},"id":"cpSRjgICoigs","outputId":"b539ed47-ef30-4e08-a2fe-f5f88d01b558"},"outputs":[{"name":"stdout","output_type":"stream","text":["Your name is Larry.\n"]}],"source":["print(chat.send_message(\"\"\"\n","What's my name again?\n","\"\"\"))"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":318,"status":"ok","timestamp":1694419899795,"user":{"displayName":"정명훈 Jerry","userId":"05756609176337112861"},"user_tz":-540},"id":"2iuK03w7SsE6","outputId":"4aeee773-8c0e-4032-976e-830333905505"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'vertexai.language_models.ChatSession'>\n"]}],"source":["print(type(chat))"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":269,"status":"ok","timestamp":1694420027849,"user":{"displayName":"정명훈 Jerry","userId":"05756609176337112861"},"user_tz":-540},"id":"NV22LAnuTI97","outputId":"ae260531-a3cc-42d2-a3c7-8923f9c2f2c3"},"outputs":[{"name":"stdout","output_type":"stream","text":["user : \n","My name is Larry.\n","\n","bot : Hi Larry, how can I help you today?\n","user : \n","What's my name again?\n","\n","bot : Your name is Larry.\n"]}],"source":["# print the chat history\n","for message in chat.message_history:\n","    # print(message)\n","    print(message.author, ':', message.content)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"X9yvwr85oigs"},"source":["### Advanced Chat model with the SDK\n","You can also provide a `context` and `examples` to the model. The model will then respond based on the provided context and examples. You can also use `temperature`, `max_output_tokens`, `top_p`, and `top_k`. These parameters should be used when you start your chat with `chat_model.start_chat()`.\n","\n","For more information on chat models, please refer to the [documentation on chat model parameters](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/models#chat_model_parameters)."]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4123,"status":"ok","timestamp":1684330705865,"user":{"displayName":"Renato Leite","userId":"17679624572242182877"},"user_tz":180},"id":"5gBhcG9Poigs","outputId":"fa07ba86-aaa3-4fe8-856e-8d346f149589"},"outputs":[{"name":"stdout","output_type":"stream","text":["Yes, your favorite movies are based on a book series.\n"]}],"source":["chat = chat_model.start_chat(\n","    context=\"My name is Ned. You are my personal assistant. My favorite movies are Lord of the Rings and Hobbit.\",\n","    examples=[\n","        InputOutputTextPair(\n","            input_text=\"Who do you work for?\",\n","            output_text=\"I work for Ned.\",\n","        ),\n","        InputOutputTextPair(\n","            input_text=\"What do I like?\",\n","            output_text=\"Ned likes watching movies.\",\n","        ),\n","    ],\n","    temperature=0.3,\n","    max_output_tokens=1024,\n","    top_p=0.8,\n","    top_k=1\n",")\n","print(chat.send_message(\"Are my favorite movies based on a book series?\"))"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"P8r5aQCioigs"},"outputs":[{"name":"stdout","output_type":"stream","text":["The books were published in 1954 and 1955.\n"]}],"source":["print(chat.send_message(\"When where these books published?\"))"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'_model': <vertexai.preview.language_models._PreviewChatModel object at 0x154149ed0>, '_context': 'My name is Ned. You are my personal assistant. My favorite movies are Lord of the Rings and Hobbit.', '_examples': [InputOutputTextPair(input_text='Who do you work for?', output_text='I work for Ned.'), InputOutputTextPair(input_text='What do I like?', output_text='Ned likes watching movies.')], '_max_output_tokens': 1024, '_temperature': 0.3, '_top_k': 1, '_top_p': 0.8, '_message_history': [ChatMessage(content='Are my favorite movies based on a book series?', author='user'), ChatMessage(content='Yes, your favorite movies are based on a book series.', author='bot'), ChatMessage(content='When where these books published?', author='user'), ChatMessage(content='The books were published in 1954 and 1955.', author='bot')], '_stop_sequences': None}\n","My name is Ned. You are my personal assistant. My favorite movies are Lord of the Rings and Hobbit.\n","[InputOutputTextPair(input_text='Who do you work for?', output_text='I work for Ned.'), InputOutputTextPair(input_text='What do I like?', output_text='Ned likes watching movies.')]\n","0.3\n","user : Are my favorite movies based on a book series?\n","bot : Yes, your favorite movies are based on a book series.\n","user : When where these books published?\n","bot : The books were published in 1954 and 1955.\n"]}],"source":["# print the chat history\n","print(chat.__dict__)\n","print(chat._context)\n","print(chat._examples)\n","print(chat._temperature)\n","for message in chat.message_history:\n","    # print(message)\n","    print(message.author, ':', message.content)"]},{"cell_type":"markdown","metadata":{},"source":["* message_history에 대화 내용이 저장되고\n","* 그 중에서 max input token을 넘지 않는 범위 내에서 input token으로 파라미터화 하여 send_message() 호출\n","* max input token(4096)을 넘어서면, 오랜 history 대화는 input token으로 전달되지 않을 수 있음"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["215858 30429\n","Nice to meet you Jerry. What can I help you with?\n","15\n","CONTEXT 0:  Your name is Jerry.\n","RESPONSE 1:  Sure, let me summarize it for you: The longest text ever is a text that is so long that it is impossible to read in one sitting. It is often used as a test of endurance or as a way to show off one's typing\n","329\n","CONTEXT 1:  Your name is Jerry.\n","RESPONSE 2:  OK, I'll summarize the following for you:\n","\n","The history of the conversation as a list of messages. Code chat does not support context.\n","835\n","CONTEXT 2:  Your name is Jerry.\n","RESPONSE 3:  I'm not able to help with that, as I'm only a language model. If you believe this is an error, please send us your feedback.\n","12550\n","CONTEXT 3:  I don't know your name. Can you tell me?\n","RESPONSE 4:  I'm not able to help with that, as I'm only a language model. If you believe this is an error, please send us your feedback.\n","30621\n","CONTEXT 4:  I don't know your name.\n"]}],"source":["def count_tokens(chat):\n","    count = 0\n","    for message in chat.message_history: \n","        count += len(message.content.split())\n","    return count\n","\n","chat = chat_model.start_chat(\n","    context=\"My name is Ned. You are my personal assistant. My favorite movies are Lord of the Rings and Hobbit.\",\n","    examples=[\n","        InputOutputTextPair(\n","            input_text=\"Can you summarize this for me: \\n The history of the conversation as a list of messages. Code chat does not support context.\",\n","            output_text=\"Sure, let me summarize it for you: Code chat does not support context.\",\n","        )\n","    ],\n","    temperature=0.3,\n","    max_output_tokens=50,\n","    top_p=0.8,\n","    top_k=3\n",")\n","\n","# read text from the url\n","import requests\n","url = \"https://rainbowfluffysheep.wordpress.com/the-longest-text-ever/\"\n","r = requests.get(url)\n","text = r.text\n","print(len(text), len(text.split()))\n","\n","# create more than 4096 tokens in msg variable\n","print(chat.send_message(\"My name is Jerry.\"))\n","print(count_tokens(chat))\n","print(\"CONTEXT 0: \", chat.send_message(\"What's my name?\"))\n","# msg = \"I am a very long message. \" * 1000\n","msg = \"Can you summarize this for me: \\n{}\".format(text[:5000])\n","print(\"RESPONSE 1: \", chat.send_message(msg))\n","print(count_tokens(chat))\n","print(\"CONTEXT 1: \", chat.send_message(\"What's my name?\"))\n","# msg = \"I am the second very long message. \" * 1000\n","msg = \"Can you summarize this for me: \\n{}\".format(text[5000:20000])\n","print(\"RESPONSE 2: \", chat.send_message(msg))\n","print(count_tokens(chat))\n","print(\"CONTEXT 2: \", chat.send_message(\"What's my name?\"))\n","# msg = \"I am the third very long message. \" * 1000\n","msg = \"Can you summarize this for me: \\n{}\".format(text[20000:100000])\n","print(\"RESPONSE 3: \", chat.send_message(msg))\n","print(count_tokens(chat))\n","print(\"CONTEXT 3: \", chat.send_message(\"What's my name?\"))\n","# msg = \"I am the forth very long message. \" * 1000\n","msg = \"Can you summarize this for me: \\n{}\".format(text[100000:])\n","print(\"RESPONSE 4: \", chat.send_message(msg))\n","print(count_tokens(chat))\n","print(\"CONTEXT 4: \", chat.send_message(\"What's my name?\"))\n","# print(chat.message_history)\n"]},{"cell_type":"markdown","metadata":{},"source":["* message history를 외부(예: DB 등)에서 가져와 사용하는 방식\n","* ChatSession을 매번 생성"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# DB에서 대화 내용을 가져온다.\n","messages = [\n","    {\n","        \"author\": \"user\", \n","        \"content\": \"My name is Jerry.\"\n","    },\n","    {\n","        \"author\": \"bot\", \n","        \"content\": \"Hi Jerry, I am your personal assistant.\"\n","    },\n","    {\n","        \"author\": \"user\",\n","        \"content\": \"What's my name?\"\n","    },\n","    {\n","        \"author\": \"bot\",\n","        \"content\": \"Your name is Jerry.\"\n","    },\n","]\n"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Your name is Jerry.\n"]}],"source":["from vertexai.language_models import ChatMessage, InputOutputTextPair\n","\n","vertex_messages = []\n","for m in messages:\n","    vertex_message = ChatMessage(content=m[\"content\"], author=m[\"author\"])\n","    vertex_messages.append(vertex_message)\n","\n","chat = chat_model.start_chat(\n","    message_history=vertex_messages,\n","    temperature=0.3,\n","    max_output_tokens=1024,\n","    top_p=0.8,\n","    top_k=1\n",")\n","\n","print(chat.send_message(\"What's my name?\"))\n","    "]}],"metadata":{"colab":{"provenance":[{"file_id":"1rIJGT2vuTFrihlqKLvpV68Uzw4E3Qbcu","timestamp":1694419100979}]},"environment":{"kernel":"python3","name":"tf2-gpu.2-11.m108","type":"gcloud","uri":"gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m108"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}
